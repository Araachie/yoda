<h1 align="center">
  <br>
	Learn the Force We Can: Multi-Object Video Generation from Pixel-Level Interactions
  <br>
</h1>
  <p align="center">
    <a href="https://araachie.github.io">Aram Davtyan</a> •
    <a href="https://www.cvg.unibe.ch/people/favaro">Paolo Favaro</a>
  </p>
<h4 align="center">Official repository of the paper</h4>

<h4 align="center"><a href="https://araachie.github.io/yoda/">Project Website</a> • <a href="https://arxiv.org/abs/00000000">Arxiv</a>

#
> **Abstract:** *We propose a novel unsupervised method to autoregressively generate videos
> from a single frame and a sparse motion input. Our trained model can generate realistic 
> object-to-object interactions and separate the dynamics and the extents of multiple objects 
> despite only observing them under correlated motion activities. Key components in our method
> are the randomized conditioning scheme, the encoding of the input motion control, and the 
> randomized and sparse sampling to break correlations. Our model, which we call YODA, has the
> ability to move objects without physically touching them. We show both qualitatively and 
> quantitatively that YODA accurately follows the user control, while yielding a video quality 
> that is on par with or better than state-of-the-art video generation prior work on several 
> datasets.*

## Citation

For citation we recommend using the following bibref.

A. Davtyan, P. Favaro. Learn the Force We Can: Multi-Object Video Generation from Pixel-Level Interactions. Technical Report, 2023.
  
## Code
  
Coming soon...
